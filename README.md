# hadoop3-cluster-docker


åˆ©ç”¨dockerå®¹å™¨æ¨¡æ‹Ÿhadoopé›†ç¾¤çš„éƒ¨ç½²ã€‚

ä½¿ç”¨æ­¥éª¤ï¼š

1. åœ¨é¡¹ç›®æ ¹ç›®å½•ä¸‹æ”¾ç½®hadoop3çš„éƒ¨ç½²åŒ…ï¼Œä¸€èˆ¬æƒ…å†µä¸‹ï¼Œå®˜æ–¹åŒ…åä¸º hadoop-x.x.x.tar.gz, å…¶ä¸­xxxä¸ºç‰ˆæœ¬å·
2. ä½¿ç”¨è„šæœ¬`build.sh`æ„å»ºé•œåƒï¼Œè„šæœ¬å†…éƒ¨çš„å‘½ä»¤`docker build -t hadoop-base --build-arg HADOOP_PACKAGE=hadoop-x.x.x .` æ„å»ºä¸€ä¸ªåä¸ºhadoop3çš„é•œåƒï¼Œæ­¤å¤„è®°å¾—å°†ç‰ˆæœ¬å·æ¢æˆè‡ªå·±æ‰€ç”¨çš„hadoopç‰ˆæœ¬å·ï¼›
3. ä¿®æ”¹etc-hadoopæ–‡ä»¶å¤¹ä¸‹çš„é…ç½®æ–‡ä»¶;
4. ä½¿ç”¨`docker-compose up` å¯åŠ¨hadoopå®¹å™¨é›†ç¾¤ï¼›

## å½“å‰é…ç½®
```Bash
# ----- core-site.xml -----
<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://hadoop01:9820</value>
    </property>
    <property>
        <name>hadoop.http.staticuser.user</name>
        <value>root</value>
    </property>
</configuration>

# ----- hdfs-site.xml -----
<configuration>
    <property>
        <name>dfs.namenode.secondary.http-address</name>
        <value>hdfs://hadoop02:9868</value>
    </property>
    <property>
        <name>dfs.replication</name>
        <value>3</value>
    </property>
    <property>
        <name>dfs.namenode.http-address</name>
        <value>hadoop01:9870</value>
    </property>
</configuration>

# ----- yarn-site.xml -----
<configuration>
    <property>
        <name>yarn.resourcemanager.hostname</name>
        <value>hadoop01</value>
    </property>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
    <property>
        <name>yarn.scheduler.minimum-allocation-mb</name>
        <value>512</value>
    </property>
    <property>
        <name>yarn.scheduler.maximum-allocation-mb</name>
        <value>2048</value>
    </property>
    <property>
        <name>yarn.nodemanager.vmem-pmem-ratio</name>
        <value>4</value>
    </property>
</configuration>

# ----- mapred-site.xml -----
<configuration>
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
    <property>
        <name>yarn.app.mapreduce.am.env</name>
        <value>HADOOP_MAPRED_HOME=${HADOOP_HOME}</value>
    </property>
    <property>
        <name>mapreduce.map.env</name>
        <value>HADOOP_MAPRED_HOME=${HADOOP_HOME}</value>
    </property>
    <property>
        <name>mapreduce.reduce.env</name>
        <value>HADOOP_MAPRED_HOME=${HADOOP_HOME}</value>
    </property>
</configuration>
# ----- hadoop-env.sh -----
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64

export HDFS_NAMENODE_USER=root
export HDFS_DATANODE_USER=root
export HDFS_SECONDARYNAMENODE_USER=root
export YARN_RESOURCEMANAGER_USER=root
export YARN_NODEMANAGER_USER=root


# ----- workers ----
hadoop01
hadoop02
hadoop03


```

å…³äºhadoopå®¹å™¨é›†ç¾¤çš„è§„åˆ’åœ¨æ–‡ä»¶docker-compose.ymlä¸­:
- hadoop01: NameNode, DataNode, NodeManager, ResourceManager, JobHistoryServer
- hadoop02: SecondaryNameNode, DataNode, NodeManager
- hadoop03: DataNode, NodeManager

æŒ‰ç…§å¦‚ä¸Šè§„åˆ’ï¼Œæ¥ä¸‹æ¥åº”è¯¥ï¼š
1. åœ¨hadoop01ä¸Šæ‰§è¡Œ`hdfs namenode -format`ï¼›
2. åœ¨hadoop01çš„${HADOOP_HOME}/sbin/ç›®å½•ä¸‹ï¼Œæ‰§è¡Œ`./start-dfs.sh`ï¼›
3. åœ¨hadoop01çš„${HADOOP_HOME}/sbin/ç›®å½•ä¸‹ï¼Œæ‰§è¡Œ`./start-yarn.sh`ï¼›
4. åœ¨hadoop01çš„${HADOOP_HOME}/sbin/ç›®å½•ä¸‹ï¼Œæ‰§è¡Œ`mapred --daemon start historyserver`ï¼›

æ¥ä¸‹æ¥å¼€å§‹æ„‰å¿«çš„hadoopä¹‹æ—…å§ï¼ğŸ˜

----- é›†ç¾¤å¯åŠ¨åœæ­¢ -----
```
start-dfs.sh     # å¯åŠ¨HDFSæ‰€æœ‰è¿›ç¨‹(NameNode, SecondaryNameNode, DataNode)
stop-dfs.sh      # åœæ­¢HDFSæ‰€æœ‰è¿›ç¨‹(NameNode, SecondaryNameNode, DataNode)

hdfs --daemon start namenode           # åªå¯åŠ¨NameNode
hdfs --daemon start secondarynamenode  # åªå¯åŠ¨SecondaryNameNode
hdfs --daemon start datanode           # åªå¯åŠ¨DataNode
# hdfs --daemon stop xxx               # åªåœæ­¢xxx

hdfs --workers --daemon start datanode   # å¯åŠ¨æ‰€æœ‰èŠ‚ç‚¹ä¸Šçš„DataNode
hdfs --workers --daemon stop datanode    # åœæ­¢æ‰€æœ‰èŠ‚ç‚¹ä¸Šçš„DataNode

```
